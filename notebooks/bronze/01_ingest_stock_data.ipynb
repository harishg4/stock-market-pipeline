{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7574b1-271b-4c5f-83be-aa92ec91abbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration - list of stocks to track\n",
    "stocks = [\"AAPL\", \"MSFT\", \"GOOGL\", \"NVDA\", \"TSLA\"]  # US stocks\n",
    "api_key = \"ZVG3CQYLUO8Q83XQ\"\n",
    "\n",
    "# Function to fetch stock data\n",
    "def fetch_stock_data(symbol):\n",
    "    url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if \"Time Series (Daily)\" not in data:\n",
    "        print(f\"Error fetching {symbol}: {data}\")\n",
    "        return []\n",
    "    \n",
    "    time_series = data[\"Time Series (Daily)\"]\n",
    "    rows = []\n",
    "    \n",
    "    for date, prices in time_series.items():\n",
    "        row = {\n",
    "            \"date\": date,\n",
    "            \"symbol\": symbol,\n",
    "            \"open\": float(prices[\"1. open\"]),\n",
    "            \"high\": float(prices[\"2. high\"]),\n",
    "            \"low\": float(prices[\"3. low\"]),\n",
    "            \"close\": float(prices[\"4. close\"]),\n",
    "            \"volume\": int(prices[\"5. volume\"]),\n",
    "            \"ingestion_timestamp\": datetime.now()\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "print(f\"Fetching data for {len(stocks)} stocks...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025042a8-85c5-419e-8a1e-cfb70d8efd4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Fetch data for all stocks\n",
    "all_rows = []\n",
    "\n",
    "for i, symbol in enumerate(stocks):\n",
    "    print(f\"Fetching {symbol} ({i+1}/{len(stocks)})...\")\n",
    "    \n",
    "    rows = fetch_stock_data(symbol)\n",
    "    all_rows.extend(rows)\n",
    "    \n",
    "    print(f\"  âœ“ Got {len(rows)} records for {symbol}\")\n",
    "    \n",
    "    # Rate limit: wait 12 seconds between API calls (5 calls per minute max)\n",
    "    if i < len(stocks) - 1:  # Don't wait after last stock\n",
    "        print(f\"  Waiting 12 seconds for rate limit...\")\n",
    "        time.sleep(12)\n",
    "\n",
    "print(f\"\\nTotal records fetched: {len(all_rows)}\")\n",
    "print(f\"Stocks: {set([r['symbol'] for r in all_rows])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d032f81-a361-4405-9aa7-60833254bd88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame from all rows\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(all_rows)\n",
    "\n",
    "print(f\"DataFrame created with {df.count()} records\")\n",
    "\n",
    "# Show sample data from each stock\n",
    "df.groupBy(\"symbol\").agg(\n",
    "    count(\"*\").alias(\"record_count\"),\n",
    "    min(\"date\").alias(\"earliest_date\"),\n",
    "    max(\"date\").alias(\"latest_date\")\n",
    ").show()\n",
    "\n",
    "# Write to bronze (using merge to avoid duplicates on re-runs)\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create database if not exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS stock_market\")\n",
    "\n",
    "# Check if table exists\n",
    "if spark.catalog.tableExists(\"stock_market.bronze_stock_data\"):\n",
    "    # Merge to handle duplicates\n",
    "    bronze_table = DeltaTable.forName(spark, \"stock_market.bronze_stock_data\")\n",
    "    \n",
    "    bronze_table.alias(\"target\").merge(\n",
    "        df.alias(\"source\"),\n",
    "        \"target.symbol = source.symbol AND target.date = source.date\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "    \n",
    "    print(\"Merged new data into existing bronze table\")\n",
    "else:\n",
    "    # First time - just write\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(\"stock_market.bronze_stock_data\")\n",
    "    print(\"Created new bronze table\")\n",
    "\n",
    "# Verify\n",
    "final_count = spark.table(\"stock_market.bronze_stock_data\").count()\n",
    "print(f\"\\nBronze table now has {final_count} total records\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_stock_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
